{"id":"ac8b1255-9b41-4a87-b49f-9d69104ae7d8","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-c4lge","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"MessagesSquare","legacy":false,"lf_version":"1.1.0","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","background_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Background Color","dynamic":false,"info":"The background color of the icon.","input_types":["Message"],"list":false,"load_from_db":false,"name":"background_color","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"chat_icon":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Icon","dynamic":false,"info":"The icon of the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"chat_icon","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        _background_color = self.background_color\n        _text_color = self.text_color\n        _icon = self.chat_icon\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\"background_color\": _background_color, \"text_color\": _text_color, \"icon\": _icon},\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Organic Apples"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"text_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Text Color","dynamic":false,"info":"The text color of the name","input_types":["Message"],"list":false,"load_from_db":false,"name":"text_color","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}}},"type":"ChatInput"},"dragging":false,"height":235,"id":"ChatInput-c4lge","position":{"x":743.9745420290319,"y":463.6977510207854},"positionAbsolute":{"x":743.9745420290319,"y":463.6977510207854},"selected":false,"type":"genericNode","width":320},{"data":{"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","id":"ParseData-npxEI","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","documentation":"","edited":false,"field_order":["data","template","sep"],"frozen":false,"icon":"braces","legacy":false,"lf_version":"1.1.0","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Text","method":"parse_data","name":"text","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"},"data":{"advanced":false,"display_name":"Data","dynamic":false,"info":"The data to convert to text.","input_types":["Data"],"list":false,"name":"data","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"other","value":""},"sep":{"advanced":true,"display_name":"Separator","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"sep","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"}}},"type":"ParseData"},"dragging":false,"height":304,"id":"ParseData-npxEI","position":{"x":1606.0595305373527,"y":751.4473696960695},"positionAbsolute":{"x":1606.0595305373527,"y":751.4473696960695},"selected":false,"type":"genericNode","width":320},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-6gk9i","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{"template":["context","question"]},"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","documentation":"","edited":false,"error":null,"field_order":["template"],"frozen":false,"full_path":null,"icon":"prompts","is_composition":null,"is_input":null,"is_output":null,"legacy":false,"lf_version":"1.1.0","metadata":{},"name":"","output_types":[],"outputs":[{"cache":true,"display_name":"Prompt Message","method":"build_prompt","name":"prompt","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"context":{"advanced":false,"display_name":"context","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"context","placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""},"question":{"advanced":false,"display_name":"question","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"question","placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"{context}\n\n---\n\nGiven the context above, answer the question as best as possible.\n\nQuestion: {question}\n\nAnswer: "}},"tool_mode":false},"type":"Prompt"},"dragging":false,"height":437,"id":"Prompt-6gk9i","position":{"x":1977.9097981422992,"y":640.5656416923846},"positionAbsolute":{"x":1977.9097981422992,"y":640.5656416923846},"selected":false,"type":"genericNode","width":320},{"data":{"id":"note-39Jdn","node":{"description":"# Vector Store RAG Overview\n\n\nThis Vector Store RAG workflow combines data ingestion and retrieval into a unified process, allowing you to manage and query your data efficiently.\n\n**Components**:\n- **ðŸ“„ Load Data**: Prepares data for vector database storage.\n  - File ingestion\n  - Text chunking\n  - Embedding generation\n  - Storage in Astra DB\n\n- **âœ¨ Retriever**: Provides intelligent search and retrieval from the vector database.\n  - User query input\n  - Database search\n  - Enhanced response generation using AI models\n\n**Workflow Instructions**:\n1. Initiate the **Load Data** flow to input your document into the vector database.\n2. Use the **Retriever** flow to conduct queries and obtain comprehensive responses based on your stored data.\n3. Adjust settings like API keys and collection names as needed for your specific use case.\n\n**Benefits**:\n- Streamlines data management with a single interface.\n- Fast, scalable vector-based search capabilities.\n- Integrates cutting-edge AI technology for rich, context-aware outputs.\n","display_name":"Read Me","documentation":"","template":{"backgroundColor":"indigo"}},"type":"note"},"dragging":false,"height":798,"id":"note-39Jdn","position":{"x":134.64227176140844,"y":307.10406179806375},"positionAbsolute":{"x":134.64227176140844,"y":307.10406179806375},"resizing":false,"selected":false,"style":{"height":798,"width":600},"type":"noteNode","width":600},{"data":{"id":"note-6MkCa","node":{"description":"## Astra DB Setup\n\n**Important Setup Information:**\n\nTo use the Astra DB component in this workflow, you'll need to obtain an Astra DB key. Follow these steps to set up your access:\n\n1. **Create an Account**: Visit [DataStax Accounts](https://accounts.datastax.com) and create an account if you don't already have one.\n2. **Generate a Key**: Once logged in, navigate to the \"Astra DB\" section to create a new application token. This token will serve as your authentication key for API access.\n3. **Configure Endpoint**: Note the API endpoint URL provided by Astra DB. This is essential for connecting your vector database to the workflow.\n4. **Input Token**: Enter the generated token and API endpoint URL in the appropriate fields in the Astra DB component within the workflow.\n\n","display_name":"","documentation":"","template":{"backgroundColor":"blue"}},"type":"note"},"dragging":false,"height":358,"id":"note-6MkCa","position":{"x":1180.1117634200216,"y":-2.4324300661269547},"positionAbsolute":{"x":1180.1117634200216,"y":-2.4324300661269547},"resizing":false,"selected":false,"style":{"height":358,"width":412},"type":"noteNode","width":412},{"data":{"id":"OpenAIModel-hCZqI","node":{"base_classes":["LanguageModel","Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Generates text using OpenAI LLMs.","display_name":"OpenAI","documentation":"","edited":false,"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed","output_parser"],"frozen":false,"icon":"OpenAI","legacy":false,"lf_version":"1.1.0","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Text","method":"text_response","name":"text_output","required_inputs":[],"selected":"Message","types":["Message"],"value":"__UNDEFINED__"},{"cache":true,"display_name":"Language Model","method":"build_model","name":"model_output","required_inputs":[],"selected":"LanguageModel","types":["LanguageModel"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","api_key":{"_input_type":"SecretStrInput","advanced":false,"display_name":"OpenAI API Key","dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","input_types":["Message"],"load_from_db":true,"name":"api_key","password":true,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":"open_api_key"},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"},"input_value":{"_input_type":"MessageInput","advanced":false,"display_name":"Input","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"json_mode":{"_input_type":"BoolInput","advanced":true,"display_name":"JSON Mode","dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","list":false,"name":"json_mode","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"max_tokens":{"_input_type":"IntInput","advanced":true,"display_name":"Max Tokens","dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","list":false,"name":"max_tokens","placeholder":"","range_spec":{"max":128000,"min":0,"step":0.1,"step_type":"float"},"required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":""},"model_kwargs":{"_input_type":"DictInput","advanced":true,"display_name":"Model Kwargs","dynamic":false,"info":"Additional keyword arguments to pass to the model.","list":false,"name":"model_kwargs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"model_name":{"_input_type":"DropdownInput","advanced":false,"combobox":false,"display_name":"Model Name","dynamic":false,"info":"","name":"model_name","options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_metadata":true,"type":"str","value":"gpt-4o-mini"},"openai_api_base":{"_input_type":"StrInput","advanced":true,"display_name":"OpenAI API Base","dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","list":false,"load_from_db":false,"name":"openai_api_base","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":""},"output_parser":{"_input_type":"HandleInput","advanced":true,"display_name":"Output Parser","dynamic":false,"info":"The parser to use to parse the output of the model","input_types":["OutputParser"],"list":false,"name":"output_parser","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"output_schema":{"_input_type":"DictInput","advanced":true,"display_name":"Schema","dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]","list":true,"name":"output_schema","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"seed":{"_input_type":"IntInput","advanced":true,"display_name":"Seed","dynamic":false,"info":"The seed controls the reproducibility of the job.","list":false,"name":"seed","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":1},"stream":{"_input_type":"BoolInput","advanced":false,"display_name":"Stream","dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","list":false,"name":"stream","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"system_message":{"_input_type":"MessageTextInput","advanced":false,"display_name":"System Message","dynamic":false,"info":"System message to pass to the model.","input_types":["Message"],"list":false,"load_from_db":false,"name":"system_message","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"temperature":{"_input_type":"FloatInput","advanced":false,"display_name":"Temperature","dynamic":false,"info":"","list":false,"name":"temperature","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"float","value":0.1}},"tool_mode":false},"type":"OpenAIModel"},"dragging":false,"height":680,"id":"OpenAIModel-hCZqI","position":{"x":2360.1432368563187,"y":571.6712358167248},"positionAbsolute":{"x":2360.1432368563187,"y":571.6712358167248},"selected":false,"type":"genericNode","width":320},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-CMfre","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Display a chat message in the Playground.","display_name":"Chat Output","documentation":"","edited":false,"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template","background_color","chat_icon","text_color"],"frozen":false,"icon":"MessagesSquare","legacy":false,"lf_version":"1.1.0","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","background_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Background Color","dynamic":false,"info":"The background color of the icon.","input_types":["Message"],"list":false,"load_from_db":false,"name":"background_color","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"chat_icon":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Icon","dynamic":false,"info":"The icon of the message.","input_types":["Message"],"list":false,"load_from_db":false,"name":"chat_icon","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, _id: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if _id:\n            source_dict[\"id\"] = _id\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    def message_response(self) -> Message:\n        _source, _icon, _display_name, _source_id = self.get_properties_from_source_component()\n        _background_color = self.background_color\n        _text_color = self.text_color\n        if self.chat_icon:\n            _icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(_source_id, _display_name, _source)\n        message.properties.icon = _icon\n        message.properties.background_color = _background_color\n        message.properties.text_color = _text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"},"data_template":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Data Template","dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","input_types":["Message"],"list":false,"load_from_db":false,"name":"data_template","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{text}"},"input_value":{"_input_type":"MessageInput","advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as output.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"sender":{"_input_type":"DropdownInput","advanced":true,"combobox":false,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_metadata":true,"type":"str","value":"Machine"},"sender_name":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"AI"},"session_id":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"text_color":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Text Color","dynamic":false,"info":"The text color of the name","input_types":["Message"],"list":false,"load_from_db":false,"name":"text_color","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}},"tool_mode":false},"type":"ChatOutput"},"dragging":false,"height":235,"id":"ChatOutput-CMfre","position":{"x":2732.8995071998584,"y":808.2967893015561},"positionAbsolute":{"x":2732.8995071998584,"y":808.2967893015561},"selected":false,"type":"genericNode","width":320},{"data":{"id":"OpenAIEmbeddings-ymdE4","node":{"base_classes":["Embeddings"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Generate embeddings using OpenAI models.","display_name":"OpenAI Embeddings","documentation":"","edited":false,"field_order":["default_headers","default_query","chunk_size","client","deployment","embedding_ctx_length","max_retries","model","model_kwargs","openai_api_key","openai_api_base","openai_api_type","openai_api_version","openai_organization","openai_proxy","request_timeout","show_progress_bar","skip_empty","tiktoken_model_name","tiktoken_enable","dimensions"],"frozen":false,"icon":"OpenAI","legacy":false,"lf_version":"1.1.0","metadata":{},"output_types":[],"outputs":[{"cache":true,"display_name":"Embeddings","method":"build_embeddings","name":"embeddings","required_inputs":[],"selected":"Embeddings","types":["Embeddings"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","chunk_size":{"_input_type":"IntInput","advanced":true,"display_name":"Chunk Size","dynamic":false,"info":"","list":false,"name":"chunk_size","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":1000},"client":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Client","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"client","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langchain_openai import OpenAIEmbeddings\n\nfrom langflow.base.embeddings.model import LCEmbeddingsModel\nfrom langflow.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIEmbeddings\"\n\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=OPENAI_EMBEDDING_MODEL_NAMES,\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\"),\n        MessageTextInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        MessageTextInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have. \"\n            \"Only supported by certain models.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            client=self.client or None,\n            model=self.model,\n            dimensions=self.dimensions or None,\n            deployment=self.deployment or None,\n            api_version=self.openai_api_version or None,\n            base_url=self.openai_api_base or None,\n            openai_api_type=self.openai_api_type or None,\n            openai_proxy=self.openai_proxy or None,\n            embedding_ctx_length=self.embedding_ctx_length,\n            api_key=self.openai_api_key or None,\n            organization=self.openai_organization or None,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            max_retries=self.max_retries,\n            timeout=self.request_timeout or None,\n            tiktoken_enabled=self.tiktoken_enable,\n            tiktoken_model_name=self.tiktoken_model_name or None,\n            show_progress_bar=self.show_progress_bar,\n            model_kwargs=self.model_kwargs,\n            skip_empty=self.skip_empty,\n            default_headers=self.default_headers or None,\n            default_query=self.default_query or None,\n        )\n"},"default_headers":{"_input_type":"DictInput","advanced":true,"display_name":"Default Headers","dynamic":false,"info":"Default headers to use for the API request.","list":false,"name":"default_headers","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"default_query":{"_input_type":"DictInput","advanced":true,"display_name":"Default Query","dynamic":false,"info":"Default query parameters to use for the API request.","list":false,"name":"default_query","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"deployment":{"_input_type":"MessageTextInput","advanced":true,"display_name":"Deployment","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"deployment","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"dimensions":{"_input_type":"IntInput","advanced":true,"display_name":"Dimensions","dynamic":false,"info":"The number of dimensions the resulting output embeddings should have. Only supported by certain models.","list":false,"name":"dimensions","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":""},"embedding_ctx_length":{"_input_type":"IntInput","advanced":true,"display_name":"Embedding Context Length","dynamic":false,"info":"","list":false,"name":"embedding_ctx_length","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":1536},"max_retries":{"_input_type":"IntInput","advanced":true,"display_name":"Max Retries","dynamic":false,"info":"","list":false,"name":"max_retries","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":3},"model":{"_input_type":"DropdownInput","advanced":false,"combobox":false,"display_name":"Model","dynamic":false,"info":"","name":"model","options":["text-embedding-3-small","text-embedding-3-large","text-embedding-ada-002"],"placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_metadata":true,"type":"str","value":"text-embedding-3-small"},"model_kwargs":{"_input_type":"DictInput","advanced":true,"display_name":"Model Kwargs","dynamic":false,"info":"","list":false,"name":"model_kwargs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"openai_api_base":{"_input_type":"MessageTextInput","advanced":true,"display_name":"OpenAI API Base","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"openai_api_base","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"openai_api_key":{"_input_type":"SecretStrInput","advanced":false,"display_name":"OpenAI API Key","dynamic":false,"info":"","input_types":["Message"],"load_from_db":true,"name":"openai_api_key","password":true,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":"open_api_key"},"openai_api_type":{"_input_type":"MessageTextInput","advanced":true,"display_name":"OpenAI API Type","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"openai_api_type","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"openai_api_version":{"_input_type":"MessageTextInput","advanced":true,"display_name":"OpenAI API Version","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"openai_api_version","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"openai_organization":{"_input_type":"MessageTextInput","advanced":true,"display_name":"OpenAI Organization","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"openai_organization","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"openai_proxy":{"_input_type":"MessageTextInput","advanced":true,"display_name":"OpenAI Proxy","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"openai_proxy","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"request_timeout":{"_input_type":"FloatInput","advanced":true,"display_name":"Request Timeout","dynamic":false,"info":"","list":false,"name":"request_timeout","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"float","value":""},"show_progress_bar":{"_input_type":"BoolInput","advanced":true,"display_name":"Show Progress Bar","dynamic":false,"info":"","list":false,"name":"show_progress_bar","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"skip_empty":{"_input_type":"BoolInput","advanced":true,"display_name":"Skip Empty","dynamic":false,"info":"","list":false,"name":"skip_empty","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"tiktoken_enable":{"_input_type":"BoolInput","advanced":true,"display_name":"TikToken Enable","dynamic":false,"info":"If False, you must have transformers installed.","list":false,"name":"tiktoken_enable","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"tiktoken_model_name":{"_input_type":"MessageTextInput","advanced":true,"display_name":"TikToken Model Name","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"tiktoken_model_name","placeholder":"","required":false,"show":true,"title_case":false,"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}},"tool_mode":false},"type":"OpenAIEmbeddings"},"dragging":false,"height":325,"id":"OpenAIEmbeddings-ymdE4","position":{"x":780.8507308775472,"y":778.2730432221887},"positionAbsolute":{"x":780.8507308775472,"y":778.2730432221887},"selected":false,"type":"genericNode","width":320},{"id":"GridGain-55wOs","type":"genericNode","position":{"x":1241.787837815171,"y":468.032147749924},"data":{"node":{"template":{"_type":"Component","csv_file":{"trace_as_metadata":true,"file_path":"","fileTypes":["csv"],"list":false,"required":false,"placeholder":"","show":true,"name":"csv_file","value":"","display_name":"CSV File","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"file","_input_type":"FileInput"},"embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"embedding","value":"","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"cache_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"name":"cache_name","value":"vector_cache","display_name":"Cache Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import TYPE_CHECKING, Optional, List, Union\nimport pandas as pd\nfrom loguru import logger\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, StrInput, FileInput\nfrom langflow.schema import Data\nfrom langchain.schema import Document\nfrom pygridgain import Client\nfrom langchain_community.vectorstores.ignite import GridGainVectorStore\n\nclass GridGainVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"GridGain Vector Store with enhanced CSV handling capabilities.\"\"\"\n\n    display_name: str = \"GridGain\"\n    description: str = \"GridGain Vector Store with CSV ingestion and search capabilities\"\n    documentation = \"https://www.gridgain.com/docs/latest/index\"\n    name = \"GridGain\"\n    icon = \"GridGain\"\n\n    inputs = [\n        StrInput(name=\"cache_name\", display_name=\"Cache Name\", required=True),\n        StrInput(name=\"host\", display_name=\"Host\", required=True),\n        IntInput(name=\"port\", display_name=\"Port\", required=True),\n        FileInput(\n            name=\"csv_file\",\n            display_name=\"CSV File\",\n            file_types=[\"csv\"],\n            required=False,\n        ),\n        HandleInput(\n            name=\"embedding\",\n            display_name=\"Embedding\",\n            input_types=[\"Embeddings\"],\n        ),\n        MessageTextInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n        ),\n    ]\n\n    def process_csv(self, csv_path: str) -> List[Document]:\n        \"\"\"Process CSV file and convert to list of document objects.\"\"\"\n        try:\n            df = pd.read_csv(csv_path)\n            documents = []\n            \n            for _, row in df.iterrows():\n                title = row.get('title', '')\n                text = row.get('text', '')\n                \n                content = f\"{title}\\n{text}\".strip()\n                \n                metadata = {\n                    \"id\": str(row.get('id', '')),\n                    \"url\": row.get('url', ''),\n                    \"vector_id\": row.get('vector_id', '')\n                }\n                \n                if content:\n                    documents.append(\n                        Document(\n                            page_content=content,\n                            metadata=metadata\n                        )\n                    )\n            \n            logger.info(f\"Processed {len(documents)} documents from CSV\")\n            return documents\n        except Exception as e:\n            logger.error(f\"Error processing CSV file: {e}\")\n            raise\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> \"GridGainVectorStore\":\n        \"\"\"Builds the GridGain Vector Store object with CSV support.\"\"\"\n        try:\n            # Connect to GridGain using provided host and port\n            client = self.connect_to_ignite(self.host, self.port)\n            logger.info(f\"Connected to GridGain at {self.host}:{self.port}\")\n\n            gridgain = GridGainVectorStore(\n                cache_name=self.cache_name,\n                embedding=self.embedding,\n                client=client\n            )\n\n            # Process CSV if provided\n            if hasattr(self, 'csv_file') and self.csv_file:\n                documents = self.process_csv(self.csv_file)\n                if documents:\n                    logger.info(f\"Adding {len(documents)} documents from CSV to GridGain\")\n                    gridgain.add_documents(documents)\n                    self.status = f\"Added {len(documents)} documents from CSV to GridGain\"\n\n            return gridgain\n\n        except Exception as e:\n            logger.error(f\"Error during GridGain Vector Store initialization: {e}\")\n            self.status = f\"Error: {str(e)}\"\n            raise\n    \n    def connect_to_ignite(self, host: str, port: int) -> Client:\n        \"\"\"Connect to GridGain server using provided host and port.\"\"\"\n        try:\n            client = Client()\n            client.connect(host, port)\n            logger.info(f\"Connected to GridGain server at {host}:{port} successfully.\")\n            return client\n        except Exception as e:\n            logger.exception(f\"Failed to connect to GridGain: {e}\")\n            raise\n\n    def search_documents(self) -> list[Data]:\n        \"\"\"Search documents with enhanced error handling.\"\"\"\n        try:\n            vector_store = self.build_vector_store()\n\n            if not self.search_query or not isinstance(self.search_query, str) or not self.search_query.strip():\n                self.status = \"No search query provided\"\n                return []\n\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = f\"Found {len(data)} results for the query: {self.search_query}\"\n            return data\n            \n        except Exception as e:\n            logger.error(f\"Error during search: {e}\")\n            self.status = f\"Search error: {str(e)}\"\n            return []","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"host":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"name":"host","value":"localhost","display_name":"Host","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"number_of_results","value":4,"display_name":"Number of Results","advanced":false,"dynamic":false,"info":"Number of results to return.","title_case":false,"type":"int","_input_type":"IntInput"},"port":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"name":"port","value":10800,"display_name":"Port","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"search_query":{"tool_mode":false,"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"GridGain Vector Store with CSV ingestion and search capabilities","icon":"GridGain","base_classes":["Data","Retriever"],"display_name":"GridGain","documentation":"https://www.gridgain.com/docs/latest/index","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true,"required_inputs":[]},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true,"required_inputs":["cache_name","host","port"]}],"field_order":["cache_name","host","port","csv_file","embedding","search_query","number_of_results"],"beta":false,"legacy":false,"edited":false,"metadata":{},"tool_mode":false,"lf_version":"1.1.0"},"type":"GridGain","id":"GridGain-55wOs"},"selected":false,"width":320,"height":788}],"edges":[{"animated":false,"className":"","data":{"sourceHandle":{"dataType":"ParseData","id":"ParseData-npxEI","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"context","id":"Prompt-6gk9i","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ParseData-npxEI{Å“dataTypeÅ“:Å“ParseDataÅ“,Å“idÅ“:Å“ParseData-npxEIÅ“,Å“nameÅ“:Å“textÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-6gk9i{Å“fieldNameÅ“:Å“contextÅ“,Å“idÅ“:Å“Prompt-6gk9iÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}","source":"ParseData-npxEI","sourceHandle":"{Å“dataTypeÅ“:Å“ParseDataÅ“,Å“idÅ“:Å“ParseData-npxEIÅ“,Å“nameÅ“:Å“textÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"Prompt-6gk9i","targetHandle":"{Å“fieldNameÅ“:Å“contextÅ“,Å“idÅ“:Å“Prompt-6gk9iÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}"},{"animated":false,"className":"","data":{"sourceHandle":{"dataType":"Prompt","id":"Prompt-6gk9i","name":"prompt","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-hCZqI","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-Prompt-6gk9i{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-6gk9iÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenAIModel-hCZqI{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-hCZqIÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","source":"Prompt-6gk9i","sourceHandle":"{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-6gk9iÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"OpenAIModel-hCZqI","targetHandle":"{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-hCZqIÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"},{"animated":false,"className":"","data":{"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-hCZqI","name":"text_output","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"ChatOutput-CMfre","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-OpenAIModel-hCZqI{Å“dataTypeÅ“:Å“OpenAIModelÅ“,Å“idÅ“:Å“OpenAIModel-hCZqIÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-ChatOutput-CMfre{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-CMfreÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","source":"OpenAIModel-hCZqI","sourceHandle":"{Å“dataTypeÅ“:Å“OpenAIModelÅ“,Å“idÅ“:Å“OpenAIModel-hCZqIÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"ChatOutput-CMfre","targetHandle":"{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-CMfreÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"},{"animated":false,"className":"","data":{"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-c4lge","name":"message","output_types":["Message"]},"targetHandle":{"fieldName":"question","id":"Prompt-6gk9i","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ChatInput-c4lge{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-c4lgeÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-6gk9i{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-6gk9iÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}","source":"ChatInput-c4lge","sourceHandle":"{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-c4lgeÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"Prompt-6gk9i","targetHandle":"{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-6gk9iÅ“,Å“inputTypesÅ“:[Å“MessageÅ“,Å“TextÅ“],Å“typeÅ“:Å“strÅ“}"},{"source":"ChatInput-c4lge","sourceHandle":"{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-c4lgeÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}","target":"GridGain-55wOs","targetHandle":"{Å“fieldNameÅ“:Å“search_queryÅ“,Å“idÅ“:Å“GridGain-55wOsÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","data":{"targetHandle":{"fieldName":"search_query","id":"GridGain-55wOs","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-c4lge","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-c4lge{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-c4lgeÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-GridGain-55wOs{Å“fieldNameÅ“:Å“search_queryÅ“,Å“idÅ“:Å“GridGain-55wOsÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}","animated":false,"className":""},{"source":"OpenAIEmbeddings-ymdE4","sourceHandle":"{Å“dataTypeÅ“:Å“OpenAIEmbeddingsÅ“,Å“idÅ“:Å“OpenAIEmbeddings-ymdE4Å“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}","target":"GridGain-55wOs","targetHandle":"{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“GridGain-55wOsÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}","data":{"targetHandle":{"fieldName":"embedding","id":"GridGain-55wOs","inputTypes":["Embeddings"],"type":"other"},"sourceHandle":{"dataType":"OpenAIEmbeddings","id":"OpenAIEmbeddings-ymdE4","name":"embeddings","output_types":["Embeddings"]}},"id":"reactflow__edge-OpenAIEmbeddings-ymdE4{Å“dataTypeÅ“:Å“OpenAIEmbeddingsÅ“,Å“idÅ“:Å“OpenAIEmbeddings-ymdE4Å“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}-GridGain-55wOs{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“GridGain-55wOsÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}","animated":false,"className":""},{"source":"GridGain-55wOs","sourceHandle":"{Å“dataTypeÅ“:Å“GridGainÅ“,Å“idÅ“:Å“GridGain-55wOsÅ“,Å“nameÅ“:Å“search_resultsÅ“,Å“output_typesÅ“:[Å“DataÅ“]}","target":"ParseData-npxEI","targetHandle":"{Å“fieldNameÅ“:Å“dataÅ“,Å“idÅ“:Å“ParseData-npxEIÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-npxEI","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"GridGain","id":"GridGain-55wOs","name":"search_results","output_types":["Data"]}},"id":"reactflow__edge-GridGain-55wOs{Å“dataTypeÅ“:Å“GridGainÅ“,Å“idÅ“:Å“GridGain-55wOsÅ“,Å“nameÅ“:Å“search_resultsÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-ParseData-npxEI{Å“fieldNameÅ“:Å“dataÅ“,Å“idÅ“:Å“ParseData-npxEIÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}","animated":false,"className":""}],"viewport":{"x":-157.8350397367933,"y":-307.47329698548185,"zoom":0.5276755564960821}},"description":"Get started with Retrieval-Augmented Generation (RAG) by ingesting data from documents and retrieving relevant chunks through vector similarity to provide contextual answers.","name":"Vector Store RAG","last_tested_version":"1.1.0","endpoint_name":null,"is_component":false}